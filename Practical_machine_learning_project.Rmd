---
title: "Weight Lifting Exercise Dataset"
subtitle: "Coursera - Practical Machine Learning Assignment"
date: "24 June 2018"
output: 
  html_document:
    keep_md: yes
  md_document:
    variant: markdown_github
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5, cache=TRUE)
options(width=120)
#rm(list = ls()) #cleaning environment
library(lattice)
library(ggplot2)
library(corrplot)
library(plyr)
library(caret)
library(randomForest)
```

# Executive Summary

The Weight Lifting Exercise Dataset will be used to build a predictive model to predict the manner in which 6 people executed exercises wearing accelerometers on the belt, forearm, arm, and dumbel while performing barbell lifts, which is given by the "classe" variable.

Building the predictive model followed these steps:

* Data cleaning and preparation,
* Exploratory data analysis,
* Pre-processing,
* Model training and validation,
* Predicting the classification in the test set.


# Data source
The data for this project come originally from this source: http://groupware.les.inf.puc-rio.br/har. [1] 

The training data for this project were prepared by Coursera and are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data which will be used to answer 20 cases are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

# Data cleaning and preparation

```{r}
train_orig <- read.csv("pml-training.csv", stringsAsFactors = FALSE, na.strings=c("", "NA", "#DIV/0!"))
test_orig <- read.csv("pml-testing.csv", stringsAsFactors = FALSE, na.strings=c("", "NA", "#DIV/0!"))
dim(train_orig) #Dimensions
dim(test_orig) #Dimensions
#head(train_orig) #excluded from knitr execution due to output size
#str(train_orig) #excluded from knitr execution due to output size
#summary(train_orig) #excluded from knitr execution due to output size
```
A lot of variables contain a great number of NA. Remove these columns first:
```{r}
rows_count <- nrow(train_orig)
max_na <- rows_count * 0.1 #90% of the data should not be NA
cols_to_remove <- colSums(is.na(train_orig)) > max_na
train_clean <- train_orig[,!cols_to_remove]
test_clean <- test_orig[,!cols_to_remove]
```

Remove other columns that we expect not to use:
```{r}
timestamps <- grep("timestamp", names(train_clean))
train_clean <- train_clean[,-c(1, timestamps)]
test_clean <- test_clean[,-c(1, timestamps)]
```

Convert character vectors to factors:
```{r}
train_clean$classe <- factor(train_clean$classe)
train_clean$user_name <- factor(train_clean$user_name)
user_name_lvls <- levels(train_clean$user_name)
train_clean$new_window <- factor(train_clean$new_window)
new_window_lvls <- levels(train_clean$new_window)
test_clean$user_name <- factor(test_clean$user_name, levels=user_name_lvls)
test_clean$new_window <- factor(test_clean$new_window, levels=new_window_lvls)
```

# Exploratory data analyses 

The test set provided by Coursera is the test set to be used for the ultimate evaluation of the model. Therefore we will only use the trainset for training and testing during model building proces.

```{r}
set.seed(11223344)
classe_col_idx <- which(names(train_clean) == "classe")
in_train <- createDataPartition(y=train_clean$classe, p=0.75, list=FALSE)
mod_train <- train_clean[in_train, ]
mod_test <- train_clean[-in_train, ]
```

Trying to find variables which are correlated to 'classe'?
```{r}
classe_corr <- cor(data.matrix(mod_train[, -classe_col_idx]), as.numeric(mod_train$classe))
classe_corr[abs(classe_corr) > 0.3,]
```

A correlation matrix might help to identify highly correlated features. During model selection they might be excluded.
```{r}
cm <- cor(data.matrix(mod_train[, -classe_col_idx])) #calculate correlation matrix
corrplot(cm, type="lower", tl.cex=0.70, tl.col="black", tl.srt = 45, diag = FALSE, method="ellipse")
```

Some features appear highly correlated.

# Pre-processing

We will use the `preProcess` function from the caret packet to apply pre-processing transformations and variable selection:
```{r}
# exclude variables that are highly correlated and
# center and scale the variables
ppm <-preProcess(mod_train[,-classe_col_idx],method=c("corr", "center", "scale"))
mod_train_pp <- predict(ppm, mod_train[,-classe_col_idx])
mod_train_pp$classe <- mod_train$classe

mod_test_pp <- predict(ppm, mod_test[,-classe_col_idx])
mod_test_pp$classe <- mod_test$classe
```

# Model training and validation

Four different models are trained and validated. Applying 5-fold cross-validation should avoid overfitting.

## Decision Tree

```{r}
dct <- NULL
start <- proc.time()
dct_train <- train(classe ~.,
                  method="rpart", #decision tree
                  data=mod_train_pp, #pre-processed train data
                  trControl=trainControl(method='cv', number=5, 
                                         allowParallel=TRUE))
dct$time <- (proc.time() - start)[3] # elapsed time
dct_train
```

## Gradient Boosting

```{r}
gbm <- NULL
start <- proc.time()
gbm_train <- train(classe ~.,
                  method="gbm", #gradient boosting
                  data=mod_train_pp, #pre-processed train data
                  trControl=trainControl(method='cv', number=5, 
                                         allowParallel=TRUE),
                  verbose=FALSE)
gbm$time <- (proc.time() - start)[3] # elapsed time
gbm_train
```

## Linear Discriminant Analysis

```{r}
lda <- NULL
start <- proc.time()
lda_train <- train(classe ~.,
                  method="lda", #linear discriminant analysis
                  data=mod_train_pp, #pre-processed train data
                  trControl=trainControl(method='cv', number=5, 
                                         allowParallel=TRUE),
                  verbose=FALSE)
lda$time <- (proc.time() - start)[3] # elapsed time
lda_train
```

## Random Forest

```{r}
rdf <- NULL
start <- proc.time()
rdf_train <- train(classe ~.,
                  method="rf", #random forest
                  data=mod_train_pp, #pre-processed train data
                  trControl=trainControl(method='cv', number=5, 
                                         allowParallel=TRUE),
                  verbose=FALSE)
rdf$time <- (proc.time() - start)[3] # elapsed time
rdf_train
```

## Model validation

Applying the fitted models on the test data set, to check accuracy and estimated out of sample error.

```{r}
dct_test <- predict(dct_train, mod_test_pp)
dct_conf_mat <- confusionMatrix(mod_test_pp$classe, dct_test)
dct_conf_mat$table
gbm_test <- predict(gbm_train, mod_test_pp)
gbm_conf_mat <- confusionMatrix(mod_test_pp$classe, gbm_test)
gbm_conf_mat$table
lda_test <- predict(lda_train, mod_test_pp)
lda_conf_mat <- confusionMatrix(mod_test_pp$classe, lda_test)
lda_conf_mat$table
rdf_test <- predict(rdf_train, mod_test_pp)
rdf_conf_mat <- confusionMatrix(mod_test_pp$classe, rdf_test)
rdf_conf_mat$table
```

Based on the confusion matrices both, the Decision Tree and the Linear Discriminant Analysis model, do not perform very well. Gradient Boosting and Random Forest show less prediction errors for the training set.

```{r}
# model accuracy and out of sample error
dct$acc <- postResample(mod_test_pp$classe, dct_test)[[1]]
dct$err <- 1 - dct$acc
gbm$acc <- postResample(mod_test_pp$classe, gbm_test)[[1]]
gbm$err <- 1 - gbm$acc
lda$acc <- postResample(mod_test_pp$classe, lda_test)[[1]]
lda$err <- 1 - lda$acc
rdf$acc <- postResample(mod_test_pp$classe, rdf_test)[[1]]
rdf$err <- 1 - rdf$acc
mod_perf <- rbind(dct,gbm,lda,rdf)
mod_perf
```

Random Forest has the highest accuracy and lowest out of sample error, followed by Gradient boosting, however both models need a long time to run. Despite the short run time Decision Tree and Linear Discriminant Analysis are not very accurate.

As we go for the highest accuracy Random Forest is the algorith of choice.

## Examination of the chosen model

```{r}
varImp(rdf_train)
```
```{r}
plot(rdf_train$finalModel, cex=0.7, main='Error vs. number of trees')
```

## Random Forest with limited number of trees

From the plot above one can see that from a numbers of trees greater than 50 the error rate does not decrease very much anymore. We therefore limit the number of trees. This should lead to a significant decrease in run time. We will compare the performance with the other models.

```{r}
rdf_n100 <- NULL
start <- proc.time()
rdf_n100_train <- train(classe ~.,
                  method="rf", #random forest
                  ntree=100,
                  data=mod_train_pp, #pre-processed train data
                  trControl=trainControl(method='cv', number=5, 
                                         allowParallel=TRUE),
                  verbose=FALSE)
rdf_n100$time <- (proc.time() - start)[3] # elapsed time
rdf_n100_train
rdf_n100_test <- predict(rdf_n100_train, mod_test_pp)
rdf_n100_conf_mat <- confusionMatrix(mod_test_pp$classe, rdf_n100_test)
rdf_n100_conf_mat$table
rdf_n100$acc <- postResample(mod_test_pp$classe, rdf_n100_test)[[1]]
rdf_n100$err <- 1 - rdf_n100$acc
mod_perf <- rbind(mod_perf, rdf_n100)
mod_perf
```

The run time is significantly reduced wiht only a slight decrease in accuracy in comparison to the 'unlimited' Random Forest model (respectively 99.75% versus 99.82% accuracy). We will use the last model to predict the 20 quiz problems.

# Apply the model to predict the 20 Quiz problems

We now can apply the model to the testing data set to answer the 20 problems from the Coursera quiz:

```{r}
test_clean_pp <- predict(ppm, test_clean) # pre-process as training data
rdf_n100_quiz <- predict(rdf_n100_train, test_clean_pp) # predict 'classe'
rdf_n100_quiz
quiz_answers <- NULL
quiz_answers <- data.frame(problem_id=test_clean_pp$problem_id, rdf_n100_quiz)
quiz_answers
```

These are the correct answers.

Just for fun we can also try to predict the quiz problems with the other models:

```{r}
dct_quiz <- predict(dct_train, test_clean_pp)
gbm_quiz <- predict(gbm_train, test_clean_pp)
lda_quiz <- predict(lda_train, test_clean_pp)
rdf_quiz <- predict(rdf_train, test_clean_pp)
data.frame(quiz_answers, dct_quiz, gbm_quiz, lda_quiz, rdf_quiz)
```

With Gradient Boosting we would have answered the questions right as well. 

# Working envorinment

```{r}
sessionInfo()
```

# References

[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. URL: http://groupware.les.inf.puc-rio.br/har
